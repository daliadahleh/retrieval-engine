{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip --quiet install whoosh\n",
    "!pip --quiet install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /resources/data/ir_data.zip, /resources/data/ir_data.zip.zip or /resources/data/ir_data.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!unzip -q /resources/data/ir_data.zip -d /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile TREC_EVAL we will later use to evaluate our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -s -w -C /resources/data/DSS_Fall2016_Assign1/trec_eval.8.1 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries: whoosh(for IR) and os,shutil(for working with files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh import index, writing\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import QueryParser\n",
    "import os, os.path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants for the paths of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENTS_DIR = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents\"\n",
    "INDEX_DIR = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/index1\"\n",
    "QUER_FILE = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/topics/air.topics\"\n",
    "QRELS_FILE = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/qrels/air.qrels\"\n",
    "OUTPUT_FILE = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/myres\"\n",
    "TREC_EVAL = \"/resources/data/DSS_Fall2016_Assign1/trec_eval.8.1/trec_eval\"\n",
    "INDEX_DIR2 = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/index2\"\n",
    "OUTPUT_FILE2 = \"/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/myres2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, define a Schema for the index\n",
    "mySchema = Schema(file_path = ID(stored=True),\n",
    "                  file_content = TEXT(analyzer = RegexTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if index exists - remove it\n",
    "if os.path.isdir(INDEX_DIR):\n",
    "    shutil.rmtree(INDEX_DIR)\n",
    "\n",
    "# create the directory for the index\n",
    "os.makedirs(INDEX_DIR)\n",
    "\n",
    "# create index\n",
    "myIndex = index.create_in(INDEX_DIR, mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email01  email03  email05  email07  email09  email14\r\n",
      "email02  email04  email06  email08  email10\r\n"
     ]
    }
   ],
   "source": [
    "# First, lets review the documents in our sample dataset\n",
    "!ls $DOCUMENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we build a list of all the full paths of the files in DOCUMENTS_DIR\n",
    "filesToIndex = []\n",
    "for root, dirs, files in os.walk(DOCUMENTS_DIR):\n",
    "    filePaths = [os.path.join(root, fileName) for fileName in files if not fileName.startswith('.')]\n",
    "    filesToIndex.extend(filePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email08\n",
      "/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email09\n",
      "/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email03\n",
      "/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email07\n",
      "/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email06\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 paths to make sure it worked\n",
    "print(\"\\n\".join(filesToIndex[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files: 11\n"
     ]
    }
   ],
   "source": [
    "# count files to index\n",
    "print(\"number of files:\", len(filesToIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already indexed: 1\n",
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "# open writer\n",
    "myWriter = writing.BufferedWriter(myIndex, period=20, limit=1000)\n",
    "\n",
    "try:\n",
    "    # write each file to index\n",
    "    for docNum, filePath in enumerate(filesToIndex):\n",
    "        with open(filePath, \"r\") as f:\n",
    "            fileContent = f.read()\n",
    "            myWriter.add_document(file_path = filePath,\n",
    "                                  file_content = fileContent)\n",
    "            \n",
    "            if (docNum % 1000 == 0):\n",
    "                print(\"already indexed:\", docNum+1)\n",
    "    print(\"done indexing.\")\n",
    "\n",
    "finally:\n",
    "    # save the index\n",
    "    myWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser = QueryParser(\"file_content\", schema=myIndex.schema)\n",
    "mySearcher = myIndex.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email01 0 2.6746417187049216\n"
     ]
    }
   ],
   "source": [
    "# run a sample query for the phrase \"item\"\n",
    "sampleQuery = myQueryParser.parse(\"item\")\n",
    "sampleQueryResults = mySearcher.search(sampleQuery, limit=None)\n",
    "\n",
    "# inspect the result:\n",
    "# for each document print the rank and the score\n",
    "for (docnum, result) in enumerate(sampleQueryResults):\n",
    "    score = sampleQueryResults.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our results we will use a topic file - a list of topics we use to evaluate our IR system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ducks\r\n",
      "02 ig nobel prizes\r\n",
      "03 mathematics\r\n",
      "04 flowing hair\r\n",
      "05 music\r\n",
      "06 AIR TV\r\n"
     ]
    }
   ],
   "source": [
    "# print the topic file\n",
    "!cat $QUER_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare our evaluate our results with a set of judged results(qrels file) using TREC_EVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 0 email01 0\r\n",
      "01 0 email02 0\r\n",
      "01 0 email03 0\r\n",
      "01 0 email04 1\r\n",
      "01 0 email05 1\r\n",
      "01 0 email06 1\r\n",
      "01 0 email07 0\r\n",
      "01 0 email08 0\r\n",
      "01 0 email09 0\r\n",
      "01 0 email10 0\r\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 lines in the qrels file\n",
    "!head -n 10 $QRELS_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a file with our results accoring to TREC_EVAL format (see assignment PDF for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load topic file - a list of topics(search phrases) used for evalutation\n",
    "topicsFile = open(QUER_FILE,\"r\")\n",
    "topics = topicsFile.read().splitlines()\n",
    "\n",
    "# create an output file to which we'll write our results\n",
    "outputTRECFile = open(OUTPUT_FILE, \"w\")\n",
    "\n",
    "# for each evaluated topic:\n",
    "# build a query and record the results in the file in TREC_EVAL format\n",
    "for topic in topics:\n",
    "    topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
    "    topicQuery = myQueryParser.parse(topic_phrase)\n",
    "    topicResults = mySearcher.search(topicQuery, limit=None)\n",
    "    for (docnum, result) in enumerate(topicResults):\n",
    "        score = topicResults.score(docnum)\n",
    "        outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
    "\n",
    "# close the topic and results file\n",
    "outputTRECFile.close()\n",
    "topicsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use TREC_EVAL to compare our results with the provided qrels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ret        \t01\t1\r\n",
      "num_rel        \t01\t3\r\n",
      "num_rel_ret    \t01\t1\r\n",
      "map            \t01\t0.3333\r\n",
      "R-prec         \t01\t0.3333\r\n",
      "bpref          \t01\t0.3333\r\n",
      "recip_rank     \t01\t1.0000\r\n",
      "ircl_prn.0.00  \t01\t1.0000\r\n",
      "ircl_prn.0.10  \t01\t1.0000\r\n",
      "ircl_prn.0.20  \t01\t1.0000\r\n",
      "ircl_prn.0.30  \t01\t1.0000\r\n",
      "ircl_prn.0.40  \t01\t0.0000\r\n",
      "ircl_prn.0.50  \t01\t0.0000\r\n",
      "ircl_prn.0.60  \t01\t0.0000\r\n",
      "ircl_prn.0.70  \t01\t0.0000\r\n",
      "ircl_prn.0.80  \t01\t0.0000\r\n",
      "ircl_prn.0.90  \t01\t0.0000\r\n",
      "ircl_prn.1.00  \t01\t0.0000\r\n",
      "P5             \t01\t0.2000\r\n",
      "P10            \t01\t0.1000\r\n",
      "P15            \t01\t0.0667\r\n",
      "P20            \t01\t0.0500\r\n",
      "P30            \t01\t0.0333\r\n",
      "P100           \t01\t0.0100\r\n",
      "P200           \t01\t0.0050\r\n",
      "P500           \t01\t0.0020\r\n",
      "P1000          \t01\t0.0010\r\n",
      "num_ret        \t05\t1\r\n",
      "num_rel        \t05\t2\r\n",
      "num_rel_ret    \t05\t0\r\n",
      "map            \t05\t0.0000\r\n",
      "R-prec         \t05\t0.0000\r\n",
      "bpref          \t05\t0.0000\r\n",
      "recip_rank     \t05\t0.0000\r\n",
      "ircl_prn.0.00  \t05\t0.0000\r\n",
      "ircl_prn.0.10  \t05\t0.0000\r\n",
      "ircl_prn.0.20  \t05\t0.0000\r\n",
      "ircl_prn.0.30  \t05\t0.0000\r\n",
      "ircl_prn.0.40  \t05\t0.0000\r\n",
      "ircl_prn.0.50  \t05\t0.0000\r\n",
      "ircl_prn.0.60  \t05\t0.0000\r\n",
      "ircl_prn.0.70  \t05\t0.0000\r\n",
      "ircl_prn.0.80  \t05\t0.0000\r\n",
      "ircl_prn.0.90  \t05\t0.0000\r\n",
      "ircl_prn.1.00  \t05\t0.0000\r\n",
      "P5             \t05\t0.0000\r\n",
      "P10            \t05\t0.0000\r\n",
      "P15            \t05\t0.0000\r\n",
      "P20            \t05\t0.0000\r\n",
      "P30            \t05\t0.0000\r\n",
      "P100           \t05\t0.0000\r\n",
      "P200           \t05\t0.0000\r\n",
      "P500           \t05\t0.0000\r\n",
      "P1000          \t05\t0.0000\r\n",
      "num_ret        \t06\t3\r\n",
      "num_rel        \t06\t2\r\n",
      "num_rel_ret    \t06\t2\r\n",
      "map            \t06\t0.8333\r\n",
      "R-prec         \t06\t0.5000\r\n",
      "bpref          \t06\t1.0000\r\n",
      "recip_rank     \t06\t1.0000\r\n",
      "ircl_prn.0.00  \t06\t1.0000\r\n",
      "ircl_prn.0.10  \t06\t1.0000\r\n",
      "ircl_prn.0.20  \t06\t1.0000\r\n",
      "ircl_prn.0.30  \t06\t1.0000\r\n",
      "ircl_prn.0.40  \t06\t1.0000\r\n",
      "ircl_prn.0.50  \t06\t1.0000\r\n",
      "ircl_prn.0.60  \t06\t0.6667\r\n",
      "ircl_prn.0.70  \t06\t0.6667\r\n",
      "ircl_prn.0.80  \t06\t0.6667\r\n",
      "ircl_prn.0.90  \t06\t0.6667\r\n",
      "ircl_prn.1.00  \t06\t0.6667\r\n",
      "P5             \t06\t0.4000\r\n",
      "P10            \t06\t0.2000\r\n",
      "P15            \t06\t0.1333\r\n",
      "P20            \t06\t0.1000\r\n",
      "P30            \t06\t0.0667\r\n",
      "P100           \t06\t0.0200\r\n",
      "P200           \t06\t0.0100\r\n",
      "P500           \t06\t0.0040\r\n",
      "P1000          \t06\t0.0020\r\n",
      "num_q          \tall\t3\r\n",
      "num_ret        \tall\t5\r\n",
      "num_rel        \tall\t7\r\n",
      "num_rel_ret    \tall\t3\r\n",
      "map            \tall\t0.3889\r\n",
      "gm_ap          \tall\t0.0141\r\n",
      "R-prec         \tall\t0.2778\r\n",
      "bpref          \tall\t0.4444\r\n",
      "recip_rank     \tall\t0.6667\r\n",
      "ircl_prn.0.00  \tall\t0.6667\r\n",
      "ircl_prn.0.10  \tall\t0.6667\r\n",
      "ircl_prn.0.20  \tall\t0.6667\r\n",
      "ircl_prn.0.30  \tall\t0.6667\r\n",
      "ircl_prn.0.40  \tall\t0.3333\r\n",
      "ircl_prn.0.50  \tall\t0.3333\r\n",
      "ircl_prn.0.60  \tall\t0.2222\r\n",
      "ircl_prn.0.70  \tall\t0.2222\r\n",
      "ircl_prn.0.80  \tall\t0.2222\r\n",
      "ircl_prn.0.90  \tall\t0.2222\r\n",
      "ircl_prn.1.00  \tall\t0.2222\r\n",
      "P5             \tall\t0.2000\r\n",
      "P10            \tall\t0.1000\r\n",
      "P15            \tall\t0.0667\r\n",
      "P20            \tall\t0.0500\r\n",
      "P30            \tall\t0.0333\r\n",
      "P100           \tall\t0.0100\r\n",
      "P200           \tall\t0.0050\r\n",
      "P500           \tall\t0.0020\r\n",
      "P1000          \tall\t0.0010\r\n"
     ]
    }
   ],
   "source": [
    "!$TREC_EVAL -q $QRELS_FILE $OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is empty? False\n",
      "Number of indexed files: 11\n"
     ]
    }
   ],
   "source": [
    "# Is it empty?\n",
    "print(\"Index is empty?\", myIndex.is_empty())\n",
    "\n",
    "# How many files indexed?\n",
    "print(\"Number of indexed files:\", myIndex.doc_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a reader object on the index\n",
    "myReader = myIndex.reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  {'file_path': '/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email08'}),\n",
       " (1,\n",
       "  {'file_path': '/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email09'}),\n",
       " (2,\n",
       "  {'file_path': '/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email03'}),\n",
       " (3,\n",
       "  {'file_path': '/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email07'}),\n",
       " (4,\n",
       "  {'file_path': '/resources/data/DSS_Fall2016_Assign1/lab1-q1-test-collection/documents/email06'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 5 indexed documents\n",
    "[(docnum, doc_dict) for (docnum, doc_dict) in myReader.iter_docs()][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Care',\n",
       " 'Carlos',\n",
       " 'Carmen',\n",
       " 'Carnivalesque',\n",
       " 'Carolina',\n",
       " 'Case',\n",
       " 'Cat',\n",
       " 'Catalysis',\n",
       " 'Catalyst',\n",
       " 'Catchers',\n",
       " 'Cater',\n",
       " 'Caused',\n",
       " 'Caveat',\n",
       " 'CbZF1d0021swQuc57kfqHt',\n",
       " 'Cechetto',\n",
       " 'Ceder',\n",
       " 'Celebratory',\n",
       " 'Center',\n",
       " 'Cereal',\n",
       " 'Ceremony',\n",
       " 'Cerrahi',\n",
       " 'Certolizumab',\n",
       " 'Cervical',\n",
       " 'Chair',\n",
       " 'Chalfie']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list indexed terms for field \"file_content\"\n",
    "[term for term in myReader.field_terms(\"file_content\")][1000:1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29729\n"
     ]
    }
   ],
   "source": [
    "#how many terms do we have?\n",
    "print(myReader.field_length(\"file_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs with 'bit' 1\n",
      "# docs with 'are' 11\n",
      "# docs with 'get' 6\n"
     ]
    }
   ],
   "source": [
    "# how many documents have the phares \"bit\", blob\"\n",
    "#   in the field \"file_content\"?\n",
    "print(\"# docs with 'bit'\", myReader.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'Text',\n",
       " 'Analysis',\n",
       " 'with',\n",
       " 'whoosh.analysis']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we start with basic tokenizer\n",
    "tokenizer = RegexTokenizer()\n",
    "[token.text for token in tokenizer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'ar', 'go', 'to', 'do', 'Text', 'Analysi', 'with', 'whoosh.analysi']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we might want use stemming:\n",
    "stmAnalyzer = RegexTokenizer() | StemFilter()\n",
    "[token.text for token in stmAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'ar', 'go', 'to', 'do', 'text', 'analysi', 'with', 'whoosh.analysi']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We probably want to lower-case it\n",
    "# so we add LowercaseFilter\n",
    "stmLwrAnalyzer = RegexTokenizer() | LowercaseFilter() | StemFilter()\n",
    "[token.text for token in stmLwrAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'do', 'text', 'analysi', 'whoosh.analysi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we probably want to ignore words like \"we\", \"are\", \"with\" when we index files\n",
    "# so we add StopFilter to filter stop words\n",
    "stmLwrStpAnalyzer = RegexTokenizer() | LowercaseFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'do', 'text', 'analysi', 'whoosh', 'analysi']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also probably want to break phrases like \"whoosh.analysis\" into \"whoosh\" and \"analysis\"\n",
    "# so we add IntraWordFilter\n",
    "stmLwrStpIntraAnalyzer = RegexTokenizer() | LowercaseFilter() | IntraWordFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpIntraAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the new analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mySchema2 = Schema(file_path = ID(stored=True),\n",
    "                   file_content = TEXT(analyzer = stmLwrStpIntraAnalyzer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if index exists - remove it\n",
    "if os.path.isdir(INDEX_DIR2):\n",
    "    shutil.rmtree(INDEX_DIR2)\n",
    "\n",
    "# create the directory for the index\n",
    "os.makedirs(INDEX_DIR2)\n",
    "\n",
    "# create index or open it if already exists\n",
    "myIndex2 = index.create_in(INDEX_DIR2, mySchema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already indexed: 1\n",
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "# open writer\n",
    "myWriter2 = writing.BufferedWriter(myIndex2, period=20, limit=1000)\n",
    "\n",
    "try:\n",
    "    # write each file to index\n",
    "    for docNum, filePath in enumerate(filesToIndex):\n",
    "        with open(filePath, \"r\") as f:\n",
    "            fileContent = f.read()\n",
    "            myWriter2.add_document(file_path = filePath,\n",
    "                                  file_content = fileContent)\n",
    "            \n",
    "            if (docNum % 1000 == 0):\n",
    "                print(\"already indexed:\", docNum+1)\n",
    "    print(\"done indexing.\")\n",
    "\n",
    "finally:\n",
    "    # save the index\n",
    "    myWriter2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser2 = QueryParser(\"file_content\", schema=myIndex2.schema)\n",
    "mySearcher2 = myIndex2.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load topic file - a list of topics(search phrases) used for evalutation\n",
    "topicsFile = open(QUER_FILE,\"r\")\n",
    "topics = topicsFile.read().splitlines()\n",
    "\n",
    "# create an output file to which we'll write our results\n",
    "outputTRECFile2 = open(OUTPUT_FILE2, \"w\")\n",
    "\n",
    "# for each evaluated topic:\n",
    "# build a query and record the results in the file in TREC_EVAL format\n",
    "for topic in topics:\n",
    "    topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
    "    topicQuery = myQueryParser2.parse(topic_phrase)\n",
    "    topicResults = mySearcher2.search(topicQuery, limit=None)\n",
    "    for (docnum, result) in enumerate(topicResults):\n",
    "        score = topicResults.score(docnum)\n",
    "        outputTRECFile2.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
    "\n",
    "# close the topic and results file\n",
    "outputTRECFile2.close()\n",
    "topicsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ret        \t01\t3\r\n",
      "num_rel        \t01\t3\r\n",
      "num_rel_ret    \t01\t3\r\n",
      "map            \t01\t1.0000\r\n",
      "R-prec         \t01\t1.0000\r\n",
      "bpref          \t01\t1.0000\r\n",
      "recip_rank     \t01\t1.0000\r\n",
      "ircl_prn.0.00  \t01\t1.0000\r\n",
      "ircl_prn.0.10  \t01\t1.0000\r\n",
      "ircl_prn.0.20  \t01\t1.0000\r\n",
      "ircl_prn.0.30  \t01\t1.0000\r\n",
      "ircl_prn.0.40  \t01\t1.0000\r\n",
      "ircl_prn.0.50  \t01\t1.0000\r\n",
      "ircl_prn.0.60  \t01\t1.0000\r\n",
      "ircl_prn.0.70  \t01\t1.0000\r\n",
      "ircl_prn.0.80  \t01\t1.0000\r\n",
      "ircl_prn.0.90  \t01\t1.0000\r\n",
      "ircl_prn.1.00  \t01\t1.0000\r\n",
      "P5             \t01\t0.6000\r\n",
      "P10            \t01\t0.3000\r\n",
      "P15            \t01\t0.2000\r\n",
      "P20            \t01\t0.1500\r\n",
      "P30            \t01\t0.1000\r\n",
      "P100           \t01\t0.0300\r\n",
      "P200           \t01\t0.0150\r\n",
      "P500           \t01\t0.0060\r\n",
      "P1000          \t01\t0.0030\r\n",
      "num_ret        \t02\t11\r\n",
      "num_rel        \t02\t8\r\n",
      "num_rel_ret    \t02\t8\r\n",
      "map            \t02\t0.9207\r\n",
      "R-prec         \t02\t0.8750\r\n",
      "bpref          \t02\t0.7500\r\n",
      "recip_rank     \t02\t1.0000\r\n",
      "ircl_prn.0.00  \t02\t1.0000\r\n",
      "ircl_prn.0.10  \t02\t1.0000\r\n",
      "ircl_prn.0.20  \t02\t1.0000\r\n",
      "ircl_prn.0.30  \t02\t1.0000\r\n",
      "ircl_prn.0.40  \t02\t1.0000\r\n",
      "ircl_prn.0.50  \t02\t1.0000\r\n",
      "ircl_prn.0.60  \t02\t0.8750\r\n",
      "ircl_prn.0.70  \t02\t0.8750\r\n",
      "ircl_prn.0.80  \t02\t0.8750\r\n",
      "ircl_prn.0.90  \t02\t0.8000\r\n",
      "ircl_prn.1.00  \t02\t0.8000\r\n",
      "P5             \t02\t0.8000\r\n",
      "P10            \t02\t0.8000\r\n",
      "P15            \t02\t0.5333\r\n",
      "P20            \t02\t0.4000\r\n",
      "P30            \t02\t0.2667\r\n",
      "P100           \t02\t0.0800\r\n",
      "P200           \t02\t0.0400\r\n",
      "P500           \t02\t0.0160\r\n",
      "P1000          \t02\t0.0080\r\n",
      "num_ret        \t03\t2\r\n",
      "num_rel        \t03\t1\r\n",
      "num_rel_ret    \t03\t1\r\n",
      "map            \t03\t1.0000\r\n",
      "R-prec         \t03\t1.0000\r\n",
      "bpref          \t03\t1.0000\r\n",
      "recip_rank     \t03\t1.0000\r\n",
      "ircl_prn.0.00  \t03\t1.0000\r\n",
      "ircl_prn.0.10  \t03\t1.0000\r\n",
      "ircl_prn.0.20  \t03\t1.0000\r\n",
      "ircl_prn.0.30  \t03\t1.0000\r\n",
      "ircl_prn.0.40  \t03\t1.0000\r\n",
      "ircl_prn.0.50  \t03\t1.0000\r\n",
      "ircl_prn.0.60  \t03\t1.0000\r\n",
      "ircl_prn.0.70  \t03\t1.0000\r\n",
      "ircl_prn.0.80  \t03\t1.0000\r\n",
      "ircl_prn.0.90  \t03\t1.0000\r\n",
      "ircl_prn.1.00  \t03\t1.0000\r\n",
      "P5             \t03\t0.2000\r\n",
      "P10            \t03\t0.1000\r\n",
      "P15            \t03\t0.0667\r\n",
      "P20            \t03\t0.0500\r\n",
      "P30            \t03\t0.0333\r\n",
      "P100           \t03\t0.0100\r\n",
      "P200           \t03\t0.0050\r\n",
      "P500           \t03\t0.0020\r\n",
      "P1000          \t03\t0.0010\r\n",
      "num_ret        \t04\t8\r\n",
      "num_rel        \t04\t5\r\n",
      "num_rel_ret    \t04\t5\r\n",
      "map            \t04\t0.8393\r\n",
      "R-prec         \t04\t0.6000\r\n",
      "bpref          \t04\t0.8400\r\n",
      "recip_rank     \t04\t1.0000\r\n",
      "ircl_prn.0.00  \t04\t1.0000\r\n",
      "ircl_prn.0.10  \t04\t1.0000\r\n",
      "ircl_prn.0.20  \t04\t1.0000\r\n",
      "ircl_prn.0.30  \t04\t1.0000\r\n",
      "ircl_prn.0.40  \t04\t1.0000\r\n",
      "ircl_prn.0.50  \t04\t1.0000\r\n",
      "ircl_prn.0.60  \t04\t1.0000\r\n",
      "ircl_prn.0.70  \t04\t0.6250\r\n",
      "ircl_prn.0.80  \t04\t0.6250\r\n",
      "ircl_prn.0.90  \t04\t0.6250\r\n",
      "ircl_prn.1.00  \t04\t0.6250\r\n",
      "P5             \t04\t0.6000\r\n",
      "P10            \t04\t0.5000\r\n",
      "P15            \t04\t0.3333\r\n",
      "P20            \t04\t0.2500\r\n",
      "P30            \t04\t0.1667\r\n",
      "P100           \t04\t0.0500\r\n",
      "P200           \t04\t0.0250\r\n",
      "P500           \t04\t0.0100\r\n",
      "P1000          \t04\t0.0050\r\n",
      "num_ret        \t05\t2\r\n",
      "num_rel        \t05\t2\r\n",
      "num_rel_ret    \t05\t0\r\n",
      "map            \t05\t0.0000\r\n",
      "R-prec         \t05\t0.0000\r\n",
      "bpref          \t05\t0.0000\r\n",
      "recip_rank     \t05\t0.0000\r\n",
      "ircl_prn.0.00  \t05\t0.0000\r\n",
      "ircl_prn.0.10  \t05\t0.0000\r\n",
      "ircl_prn.0.20  \t05\t0.0000\r\n",
      "ircl_prn.0.30  \t05\t0.0000\r\n",
      "ircl_prn.0.40  \t05\t0.0000\r\n",
      "ircl_prn.0.50  \t05\t0.0000\r\n",
      "ircl_prn.0.60  \t05\t0.0000\r\n",
      "ircl_prn.0.70  \t05\t0.0000\r\n",
      "ircl_prn.0.80  \t05\t0.0000\r\n",
      "ircl_prn.0.90  \t05\t0.0000\r\n",
      "ircl_prn.1.00  \t05\t0.0000\r\n",
      "P5             \t05\t0.0000\r\n",
      "P10            \t05\t0.0000\r\n",
      "P15            \t05\t0.0000\r\n",
      "P20            \t05\t0.0000\r\n",
      "P30            \t05\t0.0000\r\n",
      "P100           \t05\t0.0000\r\n",
      "P200           \t05\t0.0000\r\n",
      "P500           \t05\t0.0000\r\n",
      "P1000          \t05\t0.0000\r\n",
      "num_ret        \t06\t3\r\n",
      "num_rel        \t06\t2\r\n",
      "num_rel_ret    \t06\t2\r\n",
      "map            \t06\t0.8333\r\n",
      "R-prec         \t06\t0.5000\r\n",
      "bpref          \t06\t1.0000\r\n",
      "recip_rank     \t06\t1.0000\r\n",
      "ircl_prn.0.00  \t06\t1.0000\r\n",
      "ircl_prn.0.10  \t06\t1.0000\r\n",
      "ircl_prn.0.20  \t06\t1.0000\r\n",
      "ircl_prn.0.30  \t06\t1.0000\r\n",
      "ircl_prn.0.40  \t06\t1.0000\r\n",
      "ircl_prn.0.50  \t06\t1.0000\r\n",
      "ircl_prn.0.60  \t06\t0.6667\r\n",
      "ircl_prn.0.70  \t06\t0.6667\r\n",
      "ircl_prn.0.80  \t06\t0.6667\r\n",
      "ircl_prn.0.90  \t06\t0.6667\r\n",
      "ircl_prn.1.00  \t06\t0.6667\r\n",
      "P5             \t06\t0.4000\r\n",
      "P10            \t06\t0.2000\r\n",
      "P15            \t06\t0.1333\r\n",
      "P20            \t06\t0.1000\r\n",
      "P30            \t06\t0.0667\r\n",
      "P100           \t06\t0.0200\r\n",
      "P200           \t06\t0.0100\r\n",
      "P500           \t06\t0.0040\r\n",
      "P1000          \t06\t0.0020\r\n",
      "num_q          \tall\t6\r\n",
      "num_ret        \tall\t29\r\n",
      "num_rel        \tall\t21\r\n",
      "num_rel_ret    \tall\t19\r\n",
      "map            \tall\t0.7656\r\n",
      "gm_ap          \tall\t0.1364\r\n",
      "R-prec         \tall\t0.6625\r\n",
      "bpref          \tall\t0.7650\r\n",
      "recip_rank     \tall\t0.8333\r\n",
      "ircl_prn.0.00  \tall\t0.8333\r\n",
      "ircl_prn.0.10  \tall\t0.8333\r\n",
      "ircl_prn.0.20  \tall\t0.8333\r\n",
      "ircl_prn.0.30  \tall\t0.8333\r\n",
      "ircl_prn.0.40  \tall\t0.8333\r\n",
      "ircl_prn.0.50  \tall\t0.8333\r\n",
      "ircl_prn.0.60  \tall\t0.7569\r\n",
      "ircl_prn.0.70  \tall\t0.6944\r\n",
      "ircl_prn.0.80  \tall\t0.6944\r\n",
      "ircl_prn.0.90  \tall\t0.6819\r\n",
      "ircl_prn.1.00  \tall\t0.6819\r\n",
      "P5             \tall\t0.4333\r\n",
      "P10            \tall\t0.3167\r\n",
      "P15            \tall\t0.2111\r\n",
      "P20            \tall\t0.1583\r\n",
      "P30            \tall\t0.1056\r\n",
      "P100           \tall\t0.0317\r\n",
      "P200           \tall\t0.0158\r\n",
      "P500           \tall\t0.0063\r\n",
      "P1000          \tall\t0.0032\r\n"
     ]
    }
   ],
   "source": [
    "!$TREC_EVAL -q $QRELS_FILE $OUTPUT_FILE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs with 'bit' 11\n",
      "# docs with 'are' 0\n",
      "# docs with 'get' 7\n"
     ]
    }
   ],
   "source": [
    "# let count the same words again\n",
    "myReader2 = myIndex2.reader()\n",
    "print(\"# docs with 'bit'\", myReader2.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader2.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader2.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK's stemmers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/notebook/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download required resources\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll compare two stemmers and a lemmatizer\n",
    "lrStem = LancasterStemmer()\n",
    "sbStem = SnowballStemmer(\"english\")\n",
    "wnLemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a list of words to compare the stemmers on\n",
    "listWords = [\"going\", \"saying\", \"minimize\", \"maximum\", \n",
    "             \"meeting\", \"files\", \"tries\", \"is\", \"are\", \"beautiful\",\n",
    "             \"summarize\", \"better\", \"dogs\", \"phenomena\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          going              go           going              go\n",
      "            say             say          saying             say\n",
      "          minim           minim        minimize        minimize\n",
      "          maxim         maximum         maximum         maximum\n",
      "           meet            meet         meeting            meet\n",
      "            fil            file            file            file\n",
      "            tri             tri             try             try\n",
      "             is              is              is              be\n",
      "             ar             are             are              be\n",
      "         beauty          beauti       beautiful       beautiful\n",
      "           summ          summar       summarize       summarize\n",
      "            bet          better          better          better\n",
      "            dog             dog             dog             dog\n",
      "       phenomen       phenomena      phenomenon       phenomena\n"
     ]
    }
   ],
   "source": [
    "for word in listWords:\n",
    "    print(\"%15s %15s %15s %15s\" % (lrStem.stem(word),\n",
    "                                   sbStem.stem(word),\n",
    "                                   wnLemm.lemmatize(word),\n",
    "                                   wnLemm.lemmatize(word, 'v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This filter will run for both the index and the query\n",
    "from whoosh.analysis import Filter\n",
    "class CustomFilter(Filter):\n",
    "    is_morph = True\n",
    "    def __init__(self, filterFunc, *args, **kwargs):\n",
    "        self.customFilter = filterFunc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def __eq__(self):\n",
    "        return (other\n",
    "                and self.__class__ is other.__class__)\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if t.mode == 'query': # if called by query parser\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t\n",
    "            else: # == 'index' if called by indexer\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'ar', 'going', 'to', 'do', 'text', 'analys', 'with', 'whoosh.analysis']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whoosh filter for NLTK's LancasterStemmer\n",
    "myFilter1 = RegexTokenizer() | CustomFilter(LancasterStemmer().stem)\n",
    "[token.text for token in myFilter1(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'Text',\n",
       " 'Analysis',\n",
       " 'with',\n",
       " 'whoosh.analysis']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whoosh filter for NLTK's WordNetLemmatizer\n",
    "myFilter2 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize)\n",
    "[token.text for token in myFilter2(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'be', 'go', 'to', 'do', 'Text', 'Analysis', 'with', 'whoosh.analysis']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whoosh filter for NLTK's WordNetLemmatizer for verbs\n",
    "myFilter3 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize, 'v')\n",
    "[token.text for token in myFilter3(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: -h: not found\r\n"
     ]
    }
   ],
   "source": [
    "!$TREC_EVAL -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
